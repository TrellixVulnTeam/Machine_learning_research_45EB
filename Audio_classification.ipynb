{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Audio Classification using Pytorch</h2>\n",
    "<p> Author: Shubham Dash </p>\n",
    "<p>CNN based audio classification, using Mel-Spectrogram for feature map extraction</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Header imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.io import wavfile\n",
    "import glob\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Loader</h3>\n",
    "<p> Read and process the audio files using mel-spectrometer and create a Pytorch dataloader </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioRead:\n",
    "    '''\n",
    "    Data loader for loading audio dataset\n",
    "    '''\n",
    "    def __init__(self, train_path, val_path, test_path):\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.test_path = test_path\n",
    "        self.train_names = []\n",
    "        self.test_names = []\n",
    "        self.val_names = []\n",
    "    \n",
    "    def load_data(self, mode='train'):\n",
    "        '''\n",
    "        Read audio files with scipy waveread library \n",
    "        '''\n",
    "        if mode == 'train':\n",
    "            base_path = self.train_path\n",
    "        elif mode == 'val':\n",
    "            base_path = self.val_path\n",
    "        else:\n",
    "            base_path = self.test_path\n",
    "            \n",
    "        audio_list = glob.glob(pjoin(base_path, \"*.wav\"))\n",
    "        file_names = list(map(lambda x: pjoin(self.train_path, x), audio_list))\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.train_names = file_names\n",
    "        elif mode == 'val':\n",
    "            self.val_names = file_names\n",
    "        else:\n",
    "            self.test_names = file_names\n",
    "    \n",
    "    def plot_spectrogram(self, mode, S, idx):\n",
    "        '''\n",
    "        Plotting and saving Mel Spectrogram for each audio waveform\n",
    "        '''\n",
    "        if mode == 'train':\n",
    "            base_path = self.train_path\n",
    "        elif mode == 'val':\n",
    "            base_path = self.val_path\n",
    "        else:\n",
    "            base_path = self.test_path\n",
    "        file_name = 'spectrogram_' + str(mode) + '_' + str(idx) + '.jpg'\n",
    "        save_path = pjoin(base_path, file_name)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(librosa.power_to_db(S,\n",
    "                                                     ref=np.max),\n",
    "                                 y_axis='mel', fmax=8000,\n",
    "                                 x_axis='time')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    def process_data(self, mode='train'):\n",
    "        '''\n",
    "        Mel-spectrography to process raw audio files to \n",
    "        2D feature maps and save the images to .jpg\n",
    "        '''\n",
    "        if mode == 'train':\n",
    "            ans = list(map(lambda x: librosa.load(x), self.train_names))\n",
    "        elif mode == 'val':\n",
    "            ans = list(map(lambda x: librosa.load(x), self.val_names))\n",
    "        else:\n",
    "            ans = list(map(lambda x: librosa.load(x), self.test_names))\n",
    "        y, sr = list(zip(*ans)) # unzipping the lists\n",
    "        S = []\n",
    "        for i in range(len(y)):\n",
    "            s = librosa.feature.melspectrogram(y=y[i], sr=sr[i], n_mels=128,\n",
    "                                        fmax=8000)\n",
    "            S.append(s)\n",
    "            self.plot_spectrogram(mode, s, i) # plot and save Mel-spectrogram\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pytorch DataLoader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FATDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix()\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Metrics</h3>\n",
    "<p> Metrics to be used for evaluating model performance</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    \"\"\"Calculate precisions for each true class for a single sample.\n",
    "\n",
    "    Args:\n",
    "      scores: np.array of (num_classes,) giving the individual classifier scores.\n",
    "      truth: np.array of (num_classes,) bools indicating which classes are true.\n",
    "\n",
    "    Returns:\n",
    "      pos_class_indices: np.array of indices of the true classes for this sample.\n",
    "      pos_class_precisions: np.array of precisions corresponding to each of those\n",
    "        classes.\n",
    "    \"\"\"\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "    # Only calculate precisions if there are some true classes.\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "    # Retrieval list of classes for this sample.\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "    # class_rankings[top_scoring_class_index] == 0 etc.\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "    # Which of these is a true label?\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "    # Num hits for every truncated retrieval list.\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "    # Precision of retrieval list truncated at each hit, in order of pos_labels.\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def calculate_per_class_lwlrap(truth, scores):\n",
    "    \"\"\"Calculate label-weighted label-ranking average precision.\n",
    "\n",
    "    Arguments:\n",
    "      truth: np.array of (num_samples, num_classes) giving boolean ground-truth\n",
    "        of presence of that class in that sample.\n",
    "      scores: np.array of (num_samples, num_classes) giving the classifier-under-\n",
    "        test's real-valued score for each class for each sample.\n",
    "\n",
    "    Returns:\n",
    "      per_class_lwlrap: np.array of (num_classes,) giving the lwlrap for each\n",
    "        class.\n",
    "      weight_per_class: np.array of (num_classes,) giving the prior of each\n",
    "        class within the truth labels.  Then the overall unbalanced lwlrap is\n",
    "        simply np.sum(per_class_lwlrap * weight_per_class)\n",
    "    \"\"\"\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    # Space to store a distinct precision value for each class on each sample.\n",
    "    # Only the classes that are true for each sample will be filled in.\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = (\n",
    "            _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                  truth[sample_num, :]))\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = (\n",
    "            precision_at_hits)\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "    # Form average of each column, i.e. all the precisions assigned to labels in\n",
    "    # a particular class.\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    # overall_lwlrap = simple average of all the actual per-class, per-sample precisions\n",
    "    #                = np.sum(precisions_for_samples_by_classes) / np.sum(precisions_for_samples_by_classes > 0)\n",
    "    #           also = weighted mean of per-class lwlraps, weighted by class label prior across samples\n",
    "    #                = np.sum(per_class_lwlrap * weight_per_class)\n",
    "    return per_class_lwlrap, weight_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the configurations for the model\n",
    "class Config:\n",
    "    in_channels = 3\n",
    "    filter_dims = [512, 256, 128, 64]\n",
    "    out_features = 24566 # totally wrong, correct this\n",
    "    num_classes = 200\n",
    "    num_epochs = 80\n",
    "    batch_size = 64\n",
    "    test_batch_size = 256\n",
    "    lr = 3e-3\n",
    "    eta_min = 1e-5\n",
    "    t_max = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Description</h3>\n",
    "<p> Experimenting with a CNN based model for multi-class audio-tagging</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    '''\n",
    "    config is the Config object\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.cfg = config\n",
    "        self.ConvBlock = nn.Sequential(nn.Conv2d(self.cfg.in_channels, self.cfg.filter_dims[0]),\n",
    "                    nn.Conv2d(self.cfg.filter_dims[0], self.cfg.filter_dims[1]),\n",
    "                    nn.Conv2d(self.cfg.filter_dims[1], self.cfg.filter_dims[2]),\n",
    "                    nn.Conv2d(self.cfg.filter_dims[2], self.cfg.filter_dims[3]))\n",
    "        self.linear = nn.Linear(self.cfg.out_features, self.cfg.num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.ConvBlock(x)\n",
    "        out = F.softmax(self.linear(out))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traning loop\n",
    "def train(x_train, y_train, train_transforms, cfg):\n",
    "    num_epochs = 80\n",
    "    batch_size = 64\n",
    "    test_batch_size = 256\n",
    "    lr = 3e-3\n",
    "    eta_min = 1e-5\n",
    "    t_max = 10\n",
    "    \n",
    "    num_classes = y_train.shape[1]\n",
    "\n",
    "    x_trn, x_val, y_trn, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=SEED)\n",
    "    \n",
    "    train_dataset = FATTrainDataset(x_trn, y_trn, train_transforms)\n",
    "    valid_dataset = FATTrainDataset(x_val, y_val, train_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    model = Classifier(num_classes=num_classes).cuda()\n",
    "    criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "    optimizer = Adam(params=model.parameters(), lr=lr, amsgrad=False)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
    "\n",
    "    best_epoch = -1\n",
    "    best_lwlrap = 0.\n",
    "    mb = master_bar(range(num_epochs))\n",
    "\n",
    "    for epoch in mb:\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "\n",
    "        for x_batch, y_batch in progress_bar(train_loader, parent=mb):\n",
    "            preds = model(x_batch.cuda())\n",
    "            loss = criterion(preds, y_batch.cuda())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        valid_preds = np.zeros((len(x_val), num_classes))\n",
    "        avg_val_loss = 0.\n",
    "\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            preds = model(x_batch.cuda()).detach()\n",
    "            loss = criterion(preds, y_batch.cuda())\n",
    "\n",
    "            preds = torch.sigmoid(preds)\n",
    "            valid_preds[i * test_batch_size: (i+1) * test_batch_size] = preds.cpu().numpy()\n",
    "\n",
    "            avg_val_loss += loss.item() / len(valid_loader)\n",
    "            \n",
    "        score, weight = calculate_per_class_lwlrap(y_val, valid_preds)\n",
    "        lwlrap = (score * weight).sum()\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            mb.write(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  val_lwlrap: {lwlrap:.6f}  time: {elapsed:.0f}s')\n",
    "    \n",
    "        if lwlrap > best_lwlrap:\n",
    "            best_epoch = epoch + 1\n",
    "            best_lwlrap = lwlrap\n",
    "            torch.save(model.state_dict(), 'weight_best.pt')\n",
    "            \n",
    "    return {\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_lwlrap': best_lwlrap,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Driver</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_file = r'C:\\Users\\Shubham\\Desktop\\Lottery Ticket Hypothesis\\train_data'\n",
    "    d = AudioRead(train_file, train_file, train_file)\n",
    "    l, names = d.load_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
